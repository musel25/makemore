{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94deffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\").read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c1b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "charset = sorted(list(set(''.join(words))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67acff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {j:i+1  for i,j in enumerate(charset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f1f3165",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi['.']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a5af37d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '.': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8827527d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos = {i:j for j,i in stoi.items()}\n",
    "itos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accde926",
   "metadata": {},
   "source": [
    "create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8181771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c891f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)  \n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f597ea",
   "metadata": {},
   "source": [
    "One hot endoding, because they are indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c731b12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "x_enc = F.one_hot(xs, num_classes=len(stoi)).float()\n",
    "x_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daa330d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_enc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef96fbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 27])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27, 27)) # The output is also 27 dimensional, one for each character\n",
    "logits =x_enc @ W\n",
    "# probs = F.softmax(logits, dim=1)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70845e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0].sum()  # should be 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f776aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.0275, 0.0072, 0.0182, 0.0068, 0.0126, 0.0102, 0.0148, 0.0005, 0.1086,\n",
      "        0.0428, 0.0172, 0.0246, 0.0394, 0.1003, 0.0401, 0.0145, 0.0784, 0.0482,\n",
      "        0.0212, 0.0164, 0.1487, 0.0630, 0.0124, 0.0207, 0.0392, 0.0375, 0.0292])\n",
      "label (actual next character): 5\n",
      "probability assigned by the net to the the correct character: 0.010179250501096249\n",
      "log likelihood: -4.587403774261475\n",
      "negative log likelihood: 4.587403774261475\n",
      "--------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0286, 0.1065, 0.0110, 0.0136, 0.0992, 0.0090, 0.0225, 0.0532, 0.0113,\n",
      "        0.0754, 0.0494, 0.0154, 0.0164, 0.0190, 0.0202, 0.0166, 0.0368, 0.0241,\n",
      "        0.0809, 0.0505, 0.0108, 0.0236, 0.0543, 0.0106, 0.0195, 0.1169, 0.0047])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.018978804349899292\n",
      "log likelihood: -3.96443247795105\n",
      "negative log likelihood: 3.96443247795105\n",
      "--------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0751, 0.0257, 0.0186, 0.0121, 0.0909, 0.0051, 0.0153, 0.0135, 0.0433,\n",
      "        0.0032, 0.0081, 0.0081, 0.0203, 0.0126, 0.0751, 0.0245, 0.0268, 0.0708,\n",
      "        0.0062, 0.0429, 0.0690, 0.0175, 0.0088, 0.1219, 0.0046, 0.0640, 0.1161])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.012628225609660149\n",
      "log likelihood: -4.37182092666626\n",
      "negative log likelihood: 4.37182092666626\n",
      "--------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0751, 0.0257, 0.0186, 0.0121, 0.0909, 0.0051, 0.0153, 0.0135, 0.0433,\n",
      "        0.0032, 0.0081, 0.0081, 0.0203, 0.0126, 0.0751, 0.0245, 0.0268, 0.0708,\n",
      "        0.0062, 0.0429, 0.0690, 0.0175, 0.0088, 0.1219, 0.0046, 0.0640, 0.1161])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.025663506239652634\n",
      "log likelihood: -3.6626853942871094\n",
      "negative log likelihood: 3.6626853942871094\n",
      "--------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0745, 0.0420, 0.0426, 0.0159, 0.0328, 0.0362, 0.0306, 0.0312, 0.0069,\n",
      "        0.0186, 0.0783, 0.0717, 0.0160, 0.0585, 0.0163, 0.0167, 0.0441, 0.0422,\n",
      "        0.0428, 0.0086, 0.1424, 0.0465, 0.0171, 0.0068, 0.0051, 0.0170, 0.0388])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.07451629638671875\n",
      "log likelihood: -2.5967373847961426\n",
      "negative log likelihood: 2.5967373847961426\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.836616039276123\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(5)\n",
    "for i in range(5):\n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356f9aa8",
   "metadata": {},
   "source": [
    "optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37f2ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309e59b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.855605363845825\n",
      "3.446014404296875\n",
      "3.220780849456787\n",
      "3.0834012031555176\n",
      "2.9891412258148193\n",
      "2.9212939739227295\n",
      "2.8705763816833496\n",
      "2.8312137126922607\n",
      "2.799691677093506\n",
      "2.7738349437713623\n",
      "2.7522473335266113\n",
      "2.7339911460876465\n",
      "2.718406915664673\n",
      "2.7050065994262695\n",
      "2.6934139728546143\n",
      "2.68333101272583\n",
      "2.6745150089263916\n",
      "2.6667675971984863\n",
      "2.6599278450012207\n",
      "2.653859853744507\n",
      "2.6484534740448\n",
      "2.64361572265625\n",
      "2.639270782470703\n",
      "2.6353535652160645\n",
      "2.631809949874878\n",
      "2.6285951137542725\n",
      "2.6256701946258545\n",
      "2.623002529144287\n",
      "2.62056303024292\n",
      "2.6183278560638428\n",
      "2.616276264190674\n",
      "2.614389181137085\n",
      "2.6126503944396973\n",
      "2.611046314239502\n",
      "2.6095635890960693\n",
      "2.608191728591919\n",
      "2.6069204807281494\n",
      "2.605740547180176\n",
      "2.604644298553467\n",
      "2.603623867034912\n",
      "2.6026737689971924\n",
      "2.601787805557251\n",
      "2.6009607315063477\n",
      "2.6001875400543213\n",
      "2.599463939666748\n",
      "2.5987868309020996\n",
      "2.598151683807373\n",
      "2.59755539894104\n",
      "2.5969958305358887\n",
      "2.5964694023132324\n",
      "2.5959746837615967\n",
      "2.595508337020874\n",
      "2.595069169998169\n",
      "2.5946547985076904\n",
      "2.594263792037964\n",
      "2.5938944816589355\n",
      "2.59354567527771\n",
      "2.5932154655456543\n",
      "2.5929031372070312\n",
      "2.5926074981689453\n",
      "2.592327117919922\n",
      "2.5920612812042236\n",
      "2.5918092727661133\n",
      "2.5915699005126953\n",
      "2.5913431644439697\n",
      "2.591127395629883\n",
      "2.5909225940704346\n",
      "2.5907273292541504\n",
      "2.5905420780181885\n",
      "2.590365409851074\n",
      "2.5901973247528076\n",
      "2.5900375843048096\n",
      "2.5898849964141846\n",
      "2.5897395610809326\n",
      "2.5896010398864746\n",
      "2.5894689559936523\n",
      "2.5893428325653076\n",
      "2.5892221927642822\n",
      "2.5891072750091553\n",
      "2.5889976024627686\n",
      "2.588892698287964\n",
      "2.588792562484741\n",
      "2.5886967182159424\n",
      "2.5886049270629883\n",
      "2.588517189025879\n",
      "2.588433265686035\n",
      "2.588353395462036\n",
      "2.5882763862609863\n",
      "2.588202714920044\n",
      "2.588132381439209\n",
      "2.5880651473999023\n",
      "2.5880002975463867\n",
      "2.587938070297241\n",
      "2.587878942489624\n",
      "2.5878217220306396\n",
      "2.5877676010131836\n",
      "2.587714910507202\n",
      "2.5876646041870117\n",
      "2.587616443634033\n",
      "2.5875704288482666\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W # predict log-counts\n",
    "  counts = logits.exp() # counts, equivalent to N\n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  loss = -probs[torch.arange(num), ys].log().mean() + 0.1*(W**2).mean()\n",
    "  print(loss.item())\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb991ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze\n",
      "momakurailezityha\n",
      "konimittain\n",
      "llayn\n",
      "ka\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "  \n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "    \n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "    \n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    if ix == 0:\n",
    "      break\n",
    "    out.append(itos[ix])\n",
    "\n",
    "  print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "makemore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
